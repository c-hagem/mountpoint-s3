# This file (and others in `conf/`) specify some static parameters,
# as well as some which will be 'swept' over for experiments.

defaults:
    - _self_

# ===== Common parameters for all benchmarks =====
s3_bucket: ???
s3_result_bucket: !!null
application_workers: 1 # Number of concurrent workers AND number of files (j0_100GiB.bin, j1_100GiB.bin, ...)
iteration: 0
iterations: 1
run_time: 30 # Default run time in seconds
read_size: 262144 # Defaults to 256KiB, can go up to 1MiB.
read_part_size: !!null
region: "us-east-1"
write_part_size: 16777216 # 16 MiB, to allow for uploads of large files
object_size_in_gib: 100 # Size of the object to benchmark
benchmark_type: "fio" # fio, prefetch, client, client-bp, crt
s3_keys: !!null
download_checksums: true
mount_type: "stub"

# ===== Global stub/latency simulation configuration =====
# These settings apply to both libfuse stub and mountpoint stub modes
# Simulates realistic S3 request latencies using a tiered distribution approach
stub_latency:
    enabled: false # Enable latency simulation for all stub modes
    distribution: "normal" # Distribution type: currently only "normal" supported

    # Tiered latency configuration using comma-separated values:
    # Format: "default_mean,default_stddev,p90_mean,p90_stddev,p99_mean,p99_stddev,p999_mean,p999_stddev"
    #
    # Tier breakdown:
    # - 90% of requests use default tier (fast baseline)
    # - 9% use p90 tier (moderate latency)
    # - 0.9% use p99 tier (high latency)
    # - 0.1% use p99.9 tier (tail spikes)
    tiers: "180,40,400,60,650,100,2000,5000" # Default profile matching real S3 data

    # Alternative latency profiles (replace tiers value above):
    # Fast network: "100,20,200,30,350,50,800,200"     # Low latencies across all tiers
    # Slow network: "300,60,800,120,1500,300,8000,15000" # High latencies with more variance
    # High variance: "200,80,500,150,1000,300,5000,10000" # More variable for stress testing

# Global stub file configuration
# These settings control the files presented by all stub modes
#
# IMPORTANT: Number of files is ALWAYS equal to application_workers
# - This matches FIO's expectation of one file per worker
# - Files are named: j0_100GiB.bin, j1_100GiB.bin, ..., j{application_workers-1}_100GiB.bin
# - Both libfuse stub and mountpoint stub modes follow this pattern
stub_files:
    file_size_gib: !!null
        # File size in GiB (defaults to object_size_in_gib)
        # Files are named: j0_100GiB.bin, j1_100GiB.bin, j2_100GiB.bin, etc.
        # This matches FIO's filename_format=j$jobnum_${SIZE_GIB}GiB.bin

        # Network configuration


network:
    interface_names: []
    maximum_throughput_gbps:
        !!null # Monitoring options (common to all benchmarks)


monitoring:
    with_bwm: false
    with_perf_stat: false

# ===== Mountpoint configuration =====
mountpoint:
    fuse_threads: !!null
    prefix: !!null
    metadata_ttl: "indefinite"
    mountpoint_max_background: !!null
    mountpoint_congestion_threshold: !!null
    mountpoint_binary: !!null
    upload_checksums: !!null
    stub_mode: "off" # Options: "off", "fs_handler", "s3_client"
    mountpoint_debug: false
    mountpoint_debug_crt: false

# ===== Libfuse stub filesystem configuration =====
# These settings are specific to the libfuse stub binary (when mount_type: "stub")
# For file count and latency settings, see global stub_files and stub_latency sections above
stub:
    stub_binary: "./libfuse-stub/stub" # Path to the libfuse stub executable
    background_threads: 64 # Number of FUSE background threads
    read_size: 4096 # Read buffer size in bytes (C_STUB_READSIZE env var)
    # The stub binary supports additional environment variables:
    # - C_STUB_NUMFILES: Always set to application_workers value (one file per worker)
    # - C_STUB_BACKGROUND_THREADS: Set from background_threads above
    # - C_STUB_READSIZE: Set from read_size above
    # - STUB_DISTR, STUB_DISTR_MEAN, STUB_DISTR_STDDEV: Set from global stub_latency

# ===== Benchmark-specific configurations =====
benchmarks:
    fio:
        direct_io: false
        fio_benchmarks:
            - sequential_read
        fio_benchmark: "${benchmarks.fio.fio_benchmarks[0]}"
        fio_io_engine: "psync"

    prefetch:
        max_memory_target: !!null

    crt:
        crt_benchmarks_path: !!null

    client:
        # None

    client_backpressure:
        read_window_size: !!null #2147483648


hydra:
    help:
        app_name: "Mountpoint benchmark runner"
    mode: MULTIRUN
    job:
        chdir: true
    sweeper:
        # Global sweeper params - use this for common parameters across all benchmarks
        # For specific parameter use sweep params under the specific benchmark type config
        params:
            # application_workers controls BOTH the number of concurrent workers AND number of files
            # e.g., 4 workers = files j0_100GiB.bin, j1_100GiB.bin, j2_100GiB.bin, j3_100GiB.bin
            "application_workers": 1, 4, 16, 64, 128
            "iteration": "range(${iterations})"
            "mountpoint.fuse_threads": 1, 16, 64
            "benchmarks.fio.direct_io": false, true
            # 'benchmarks.prefetch.max_memory_target': !!null, 1073741824, 2147483648  # null, 1GB, 2GB
            #'benchmarks.client_backpressure.read_window_size': 8388608, 2147483648
